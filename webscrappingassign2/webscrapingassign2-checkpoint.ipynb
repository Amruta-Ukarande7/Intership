{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first install the selenium library\n",
    "! pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now import all the required libraries.\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location using selenium. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.naukri.com\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job search bar\n",
    "search_job = driver.find_element_by_xpath(\"//input[@class='sugInp']\")\n",
    "search_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write on search bar\n",
    "search_job.send_keys('Data Analyst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job location bar\n",
    "search_loc=driver.find_element_by_id('qsb-location-sugg')\n",
    "search_loc.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn= driver .find_element_by_xpath(\"//button[@class='btn']\")\n",
    "search_btn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking  using xpath function\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='btn']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so let's extract all the tags having the job titles\n",
    "title_tags=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "title_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the text of the job title  from the tags\n",
    "job_titles=[]\n",
    "for i in title_tags:\n",
    "    if i.text is None:\n",
    "        job_titles.append('Not')\n",
    "    else:\n",
    "        job_titles.append(i.text)\n",
    "job_titles[:10]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets extract all the tags having company names\n",
    "company_tags=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "company_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will extract the text from the tags by looping over these tags\n",
    "companies_names=[]\n",
    "\n",
    "for i in company_tags:\n",
    "    companies_names.append(i.text)\n",
    "companies_names[:10]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so lets extract  all the tags having the experience required data\n",
    "experience_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience'] //span\")\n",
    "experience_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no we will extract the text  from these tags only by one by looping over these tags\n",
    "experience_list=[]\n",
    "for i in experience_tags:\n",
    "    experience_list.append(i.text)\n",
    "experience_list[:10]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_tags=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span\")\n",
    "locations_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_list=[]\n",
    "for i in locations_tags:\n",
    "    locations_list.append(i.text)\n",
    "locations_list[:10]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So lets check th length of ech element.\n",
    "print(len(job_titles[:10])),print(len(companies_names[:10])),print(len(experience_list[:10])),print(len(locations_list[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs=pd.DataFrame({})\n",
    "jobs['title']=job_titles[:10]\n",
    "jobs['company']=companies_names[:10]\n",
    "jobs['experience_required']=experience_list[:10]\n",
    "jobs['location']=locations_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have created the datset named jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.naukri.com\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job search bar\n",
    "search_job = driver.find_element_by_xpath(\"//input[@class='sugInp']\")\n",
    "search_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write on search bar\n",
    "search_job.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job location bar\n",
    "search_loc=driver.find_element_by_id('qsb-location-sugg')\n",
    "search_loc.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn= driver .find_element_by_xpath(\"//button[@class='btn']\")\n",
    "search_btn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking  using xpath function\n",
    "search_btn=driver.find_element_by_class_name('btn')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so let's extract all the tags having the job titles\n",
    "title_tag=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "title_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the text of the job title  from the tags\n",
    "job1_titles=[]\n",
    "for i in title_tag:\n",
    "    if i.text is None:\n",
    "        job1_titles.append('Not')\n",
    "    else:\n",
    "        job1_titles.append(i.text)\n",
    "job1_titles[:10]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets extract all the tags having company names\n",
    "company_tag=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "company_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will extract the text from the tags by looping over these tags\n",
    "companies1_names=[]\n",
    "\n",
    "for i in company_tag:\n",
    "    companies1_names.append(i.text)\n",
    "companies1_names[:10]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets extract all the tags having locations\n",
    "locations_tag=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span\")\n",
    "locations_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations1_list=[]\n",
    "for i in locations_tag:\n",
    "    locations1_list.append(i.text)\n",
    "locations1_list[:10]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(job1_titles[:10])),print(len(companies1_names[:10])),print(len(locations1_list[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(r\"C:\\Users\\Neha\\Downloads\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver.get('https://www.naukri.com/data-scientist-jobs-in-banglore-bagaluru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\"):\n",
    "    urls.append(i.get_attribute(\"href\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in urls[:10]:\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        description=driver.find_element_by_xpath(\"//section[@class='job-desc']\").text\n",
    "        job_description.append(description)\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        job_description.append(\"Not Available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(job_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job1=pd.DataFrame({})\n",
    "job1['title']=job1_titles[:10]\n",
    "job1['company_name']=companies1_names[:10]\n",
    "job1['location']=locations1_list[:10]\n",
    "job1['job_desc']=job_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we can see that we have created the dataset for Data scientist jobs named job1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3: In this question you have to scrape data using selenium the filters available on the webpage as shown below:\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company name, experience required.\n",
    "The location filter to be used is “Delhi/NCR”. The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill, Designations, and Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "5. Then scrape the data for the first 10 jobs results you get.\n",
    "6. Finally create a dataframe of the scraped data.\n",
    "Note: All of the above steps have to be done in code. No step is to be done manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.naukri.com\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job search bar\n",
    "search_job = driver.find_element_by_xpath(\"//input[@class='sugInp']\")\n",
    "search_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write on search bar\n",
    "search_job.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn= driver .find_element_by_xpath(\"//button[@class='btn']\")\n",
    "search_btn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clicking  using xpath function\n",
    "search_btn=driver.find_element_by_class_name('btn')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#so let's extract all the tags having the job titles\n",
    "title_t1=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "title_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the text of the job title  from the tags\n",
    "job_titles=[]\n",
    "for i in title_t1:\n",
    "    if i.text is None:\n",
    "        job_titles.append('Not')\n",
    "    else:\n",
    "        job_titles.append(i.text)\n",
    "job_titles[:10]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets extract all the tags having company names\n",
    "company_t1=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "company_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will extract the text from the tags by looping over these tags\n",
    "companies_names=[]\n",
    "\n",
    "for i in company_t1:\n",
    "    companies_names.append(i.text)\n",
    "companies_names[:10]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so lets extract  all the tags having the experience required data\n",
    "experience_t1=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience'] //span\")\n",
    "experience_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no we will extract the text  from these tags only by one by looping over these tags\n",
    "experience_list=[]\n",
    "for i in experience_t1:\n",
    "    experience_list.append(i.text)\n",
    "experience_list[:10]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So lets extract all the tags having locations\n",
    "locations_t1=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span\")\n",
    "locations_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we wil extract the text from these tags only by one by looping over these tags\n",
    "locations_list=[]\n",
    "for i in locations_t1:\n",
    "    locations_list.append(i.text)\n",
    "locations_list[:10]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#So lets check th length of ech element.\n",
    "print(len(job_titles[:10])),print(len(companies_names[:10])),print(len(experience_list[:10])),print(len(locations_list[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs2=pd.DataFrame({})\n",
    "jobs2['title']=job_titles[:10]\n",
    "jobs2['company']=companies_names[:10]\n",
    "jobs2['experience_required']=experience_list[:10]\n",
    "jobs2['location']=locations_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have created the dataset named jobs2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are getting response 403 . so we can't scrape the data from this website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here also we are getting respponse 403."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "The attributes which you have to scrape is ticked marked in the below image.\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to Flipkart webpage by url : https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and\n",
    "click the search icon\n",
    "3. After that you will reach to the page having a lot of sunglasses. From this page you can scrap the\n",
    "required data as usual.\n",
    "4. After scraping data from the first page, go to the “Next” Button at the bottom other page , then\n",
    "click on it.\n",
    "5. Now scrape data from this page as usual\n",
    "6. Repeat this until you get data for 100 sunglasses.\n",
    "Note: That all of the above steps have to be done by coding only and not manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.flipkart.com/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job search bar\n",
    "search_g= driver.find_element_by_xpath(\"//input[@type='text']\")\n",
    "search_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write on search bar\n",
    "search_g.send_keys('sunglasses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn=driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search_btn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn=driver.find_element_by_class_name('L0Z3Pu')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_name=[]\n",
    "Price=[]\n",
    "P_desc=[]\n",
    "Discount=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    b_name=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "    p_desc=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "    price =driver.find_elements_by_xpath(\"//div[@class='_25b18c']\")\n",
    "    discount=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']\")\n",
    "    \n",
    "    for j  in b_name:\n",
    "        B_name.append(j.text)\n",
    "    B_name[:100]    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for k in p_desc:\n",
    "        P_desc.append(k.text)\n",
    "    P_desc[:100] \n",
    "    \n",
    "    \n",
    "    for l in price:\n",
    "        Price.append(l.text)\n",
    "    Price[:100] \n",
    "    \n",
    "    \n",
    "    for t in discount:\n",
    "        Discount.append(t.text)\n",
    "    Discount[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_name[:100]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(B_name[:100])),print(len(Price[:100])),print(len(P_desc[:100])),print(len(Discount[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sun_gl=pd.DataFrame({})\n",
    "sun_gl['Brand_name']=B_name[:100]\n",
    "sun_gl['P_price']=Price[:100]\n",
    "sun_gl['Pr_desc']=P_desc[:100]\n",
    "sun_gl['P_discount']=Discount[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sun_gl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#We can see that we have created the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the\n",
    "search field.\n",
    "You have to scrape 3 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "As shown in the below image, you have to scrape the above attributes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.flipkart.com/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job search bar\n",
    "search_g= driver.find_element_by_xpath(\"//input[@type='text']\")\n",
    "search_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write on search bar\n",
    "search_g.send_keys('sneakers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn=driver.find_element_by_xpath(\"//button[@class='L0Z3Pu']\")\n",
    "search_btn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn=driver.find_element_by_class_name('L0Z3Pu')\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_name=[]\n",
    "Price=[]\n",
    "P_desc=[]\n",
    "Discount=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    b_name=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "    p_desc=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "    price =driver.find_elements_by_xpath(\"//div[@class='_25b18c']\")\n",
    "    discount=driver.find_elements_by_xpath(\"//div[@class='_3Ay6Sb']\")\n",
    "    \n",
    "    for j  in b_name:\n",
    "        B_name.append(j.text)\n",
    "    B_name[:100]    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for k in p_desc:\n",
    "        P_desc.append(k.text)\n",
    "    P_desc[:100] \n",
    "    \n",
    "    \n",
    "    for l in price:\n",
    "        Price.append(l.text)\n",
    "    Price[:100] \n",
    "    \n",
    "    \n",
    "    for t in discount:\n",
    "        Discount.append(t.text)\n",
    "    Discount[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(B_name[:100])),print(len(Price[:100])),print(len(P_desc[:100])),print(len(Discount[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sun_gl=pd.DataFrame({})\n",
    "sun_gl['Brand_name']=B_name[:100]\n",
    "sun_gl['P_price']=Price[:100]\n",
    "sun_gl['Pr_desc']=P_desc[:100]\n",
    "sun_gl['P_discount']=Discount[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sun_gl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7: Go to webpage https://www.amazon.in/ Enter “Laptop” in the search field and then click the search icon. Then\n",
    "set CPU Type filter to “Intel Core i7” as shown in the below image:\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "1. Title\n",
    "2. Ratings\n",
    "3. Price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\" https://www.amazon.in \"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding element for job search bar\n",
    "search_g= driver.find_element_by_xpath(\"//input[@type='text']\")\n",
    "search_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write on search bar\n",
    "search_g.send_keys('Laptop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn=driver.find_element_by_xpath(\"//input[@id='nav-search-submit-button']\")\n",
    "search_btn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn=driver.find_element_by_xpath(\"//input[@id='nav-search-submit-button']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No core filter are shown on the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Title=[]\n",
    "Price=[]\n",
    "Rating=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    b_name=driver.find_elements_by_xpath(\"//div[@class='_2WkVRV']\")\n",
    "    p_desc=driver.find_elements_by_xpath(\"//a[@class='IRpwTa']\")\n",
    "    price =driver.find_elements_by_xpath(\"//div[@class='_25b18c']\")\n",
    "    \n",
    "    \n",
    "    for j  in b_name:\n",
    "        Title.append(j.text)\n",
    "    Title[:100]    \n",
    "    \n",
    "    \n",
    "    \n",
    "    for k in p_desc:\n",
    "        P_desc.append(k.text)\n",
    "    P_desc[:100] \n",
    "    \n",
    "    \n",
    "    for l in price:\n",
    "        Price.append(l.text)\n",
    "    Price[:100] \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
